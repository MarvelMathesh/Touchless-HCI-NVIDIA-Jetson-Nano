# ============================================================================
# Touchless Media Control - Edge AI System Configuration
# Optimized for NVIDIA Jetson Nano (ARM Cortex-A57 + Maxwell GPU)
# ============================================================================

system:
  name: "Touchless Media Control"
  version: "2.0.0"
  mode: "control"  # control | demo | benchmark | calibration

# --- Camera Configuration ---
camera:
  device_id: 0
  width: 640
  height: 480
  fps: 30
  backend: "v4l2"           # v4l2 | gstreamer | auto
  buffer_size: 1            # Minimize latency (1 frame buffer)
  flip_horizontal: true     # Mirror mode for intuitive control
  auto_exposure: true
  warmup_frames: 15         # Frames to discard on startup
  capture_timeout_ms: 100

# --- MediaPipe Hand Detection ---
# Note: model_complexity is only available in MediaPipe >= 0.8.9
# On Jetson Nano with MediaPipe 0.8.5, it will be silently ignored
mediapipe:
  model_complexity: 0       # 0=Lite (fastest), 1=Full (ignored on 0.8.5)
  max_num_hands: 2          # Support multi-user
  min_detection_confidence: 0.6
  min_tracking_confidence: 0.5
  static_image_mode: false  # Video stream mode (faster)

# --- Gesture Recognition ---
recognition:
  # Static gesture thresholds (per-gesture tuning)
  # v2: Lowered to match new classifier score range (base 0.50 + curl bonuses)
  confidence_thresholds:
    thumbs_up: 0.65
    peace_sign: 0.62
    ok_sign: 0.60
    fist: 0.65
    open_palm: 0.60
    finger_point: 0.58
    thumbs_down: 0.63
    default: 0.58

  # Temporal filtering
  temporal:
    window_size: 5           # Frames for consensus voting
    consensus_ratio: 0.6     # 60% agreement needed
    recency_weight: 0.2      # Higher = more weight on recent frames

  # Dynamic gesture detection
  dynamic:
    min_velocity: 200        # pixels/sec for swipe detection
    trajectory_window: 10    # Frames to track for trajectory
    swipe_min_distance: 80   # Minimum pixels for swipe
    circle_min_points: 15    # Minimum points for circle detection

  # Dwell time for static gestures (ms)
  dwell_time_ms: 400

# --- Debouncing & Action Control ---
debouncing:
  cooldown_ms: 500           # Minimum time between different actions
  repeat_cooldown_ms: 300    # Cooldown for same action repeat
  hold_repeat_ms: 200        # Rate for held actions (volume)
  max_queue_size: 5          # Action queue limit

# --- Media Control (VLC) ---
media:
  player: "vlc"
  control_method: "xdotool"  # xdotool | dbus | python-vlc
  keybindings:
    play_pause: "space"
    volume_up: "plus"
    volume_down: "minus"
    seek_forward: "Right"
    seek_backward: "Left"
    mute: "m"
    fullscreen: "f"
    quit_player: "q"
  seek_seconds: 10
  volume_step: 5

# --- User Adaptation Engine ---
adaptation:
  enabled: true
  calibration_duration_sec: 30
  profile_storage: "data/profiles"
  max_profiles: 10
  learning_rate: 0.1
  adaptation_window: 50      # Recent gestures to consider
  face_detection_for_user_id: false  # Use face for user switching

# --- Performance Optimization ---
performance:
  target_fps: 28
  max_latency_ms: 30
  enable_gpu: true
  enable_threading: true
  thermal_throttle_temp: 70   # Celsius - reduce FPS
  thermal_critical_temp: 80   # Celsius - CPU-only mode
  frame_skip_threshold: 50    # ms - skip frame if processing too slow
  metrics_window: 100         # Frames for rolling average

# --- Visualization ---
visualization:
  enabled: true
  show_landmarks: true
  show_connections: true
  show_fps: true
  show_latency: true
  show_gesture: true
  show_confidence_bar: true
  show_gesture_legend: true
  show_hand_bbox: true
  window_name: "Touchless Media Control"

  colors:
    landmark: [0, 255, 0]       # Green
    connection: [255, 255, 255]  # White
    bbox: [0, 255, 255]         # Yellow
    text: [255, 255, 255]       # White
    fps_good: [0, 255, 0]       # Green (>25 FPS)
    fps_warn: [0, 255, 255]     # Yellow (15-25 FPS)
    fps_bad: [0, 0, 255]        # Red (<15 FPS)
    confidence_high: [0, 255, 0]
    confidence_mid: [0, 255, 255]
    confidence_low: [0, 0, 255]
    action_feedback: [255, 200, 0]

  dashboard:
    opacity: 0.7
    position: "top"   # top | bottom
    height: 80

# --- Logging ---
logging:
  level: "INFO"          # DEBUG | INFO | WARNING | ERROR
  file: "logs/system.log"
  max_size_mb: 10
  backup_count: 3
  log_performance: true
  log_gestures: true
  log_actions: true

# --- Data Collection ---
data_collection:
  output_dir: "data/gestures"
  image_format: "jpg"
  save_landmarks: true
  save_metadata: true
  samples_per_gesture: 400
  auto_augment: true

# --- ML Classifier (Phase 3) ---
ml_classifier:
  enabled: true              # Set false to force rule-based only
  model_dir: "models/weights"
  trt_engine: "gesture_net.engine"   # TensorRT FP16 engine (fastest)
  onnx_model: "gesture_net.onnx"     # ONNX model (fallback)
  pth_model: "gesture_net.pth"       # PyTorch checkpoint (fallback)
  confidence_threshold: 0.60         # Min ML confidence to accept
  fallback_on_low_confidence: true   # Fall back to rules when ML is unsure
  # Training parameters (used by training/train.py)
  training:
    epochs: 80
    batch_size: 32
    learning_rate: 0.001
    weight_decay: 0.0001
    val_split: 0.2
    patience: 15
    fp16: true               # TensorRT FP16 precision
